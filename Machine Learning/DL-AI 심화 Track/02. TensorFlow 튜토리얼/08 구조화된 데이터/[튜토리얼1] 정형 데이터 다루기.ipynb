{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c05P9g5WjizZ"
   },
   "source": [
    "# [튜토리얼1] 정형 데이터 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K1y4OHpGgss7"
   },
   "source": [
    "이 튜토리얼은 **정형 데이터(structured data)** 를 다루는 방법을 소개합니다. \n",
    "\n",
    "정형 데이터란 구조화된 데이터를 의미하며,예를 들어 CSV에서 읽은 표 형식의 데이터가 이에 해당합니다. 정형 데이터를 다루기 위해 [케라스(Keras) API](https://www.tensorflow.org/guide/keras)를 사용하여 모델을 정의하고 [피쳐 열](https://www.tensorflow.org/api_docs/python/tf/feature_column)(feature column)을 사용하여 CSV의 열을 모델 훈련에 필요한 피쳐로 매핑하겠습니다. \n",
    "\n",
    "이 튜토리얼은 다음 내용을 포함합니다:\n",
    "\n",
    "* [판다스](https://pandas.pydata.org/)(Pandas)를 사용하여 CSV 파일을 읽기\n",
    "* [tf.data](https://www.tensorflow.org/guide/datasets)를 사용하여 행을 섞고 배치로 나누는 입력 파이프라인(pipeline) 만들기\n",
    "* CSV의 열을 피쳐 열(`feature_column`)을 사용해 모델 훈련에 필요한 피쳐로 매핑하기\n",
    "* 케라스(Keras)를 사용하여 모델 구축, 훈련, 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9dEreb4QKizj"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZkLStSpzWUK"
   },
   "source": [
    "# 목차\n",
    "1. 데이터셋\n",
    "2. 판다스로 데이터프레임 만들기\n",
    "3. 데이터프레임을 훈련 세트, 검증 세트, 테스트 세트로 나누기\n",
    "4. tf.data를 사용하여 입력 파이프라인 만들기\n",
    "5. 입력 파이프라인 이해하기\n",
    "6. 여러 종류의 피쳐 열 알아보기\n",
    "7. 사용할 열 선택하기\n",
    "8. 모델 생성, 컴파일, 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터셋\n",
    "\n",
    "클리블랜드(Cleveland) 심장병 재단에서 제공한 작은 [CSV 데이터셋](https://archive.ics.uci.edu/ml/datasets/heart+Disease)을 사용하겠습니다. 이 CSV 파일은 수백 개의 행으로 이루어져 있습니다. 각 행은 환자 한 명을 나타내고 각 열은 해당 환자에 대한 속성 값을 나타냅니다. 이 정보를 사용해 환자의 심장병 발병 여부를 예측해 보겠습니다. 즉 이 데이터셋은 이진 분류 문제입니다.\n",
    "\n",
    "다음은 데이터셋에 대한 [설명](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names)입니다. 데이터셋 내에 **수치형과 범주형 열**이 모두 있다는 점을 주목하세요.\n",
    "\n",
    ">열| <center>설명</center>| 피쳐 타입 | 데이터 타입\n",
    ">------------|: -------------------- :|----------------------|-----------------\n",
    ">Age | <center>나이</center> | 수치형 | 정수\n",
    ">Sex | <center>성별(1 = 남성; 0 = 여성)</center> | 범주형 | 정수\n",
    ">CP | <center>가슴 통증 유형 (0, 1, 2, 3, 4)</center> | 범주형 | 정수\n",
    ">Trestbpd | <center>안정 혈압 (병원 입원시 mm Hg)</center> | 수치형 | 정수\n",
    ">Chol | <center>혈청 콜레스테롤 (mg/dl)</center> | 수치형 | 정수\n",
    ">FBS | <center>공복 혈당 > 120 mg/dl (1 = true; 0 = false)</center> | 범주형 | 정수\n",
    ">RestECG | <center>안정 심전도 결과 (0, 1, 2)</center> | 범주형 | 정수\n",
    ">Thalach | <center>최대 심박동수</center> | 수치형 | 정수\n",
    ">Exang | <center>협심증 유발 운동 (1 = yes; 0 = no)</center> | 범주형 | 정수\n",
    ">Oldpeak | <center>비교적 안정되기까지 운동으로 유발되는 ST depression</center> | 수치형 | 정수\n",
    ">Slope | <center>최대 운동 ST segment의 기울기</center> | 수치형 | 실수\n",
    ">CA | <center>형광 투시된 주요 혈관의 수 (0-3)</center> | 수치형 | 정수\n",
    ">Thal | <center>심장에 혈류를 측정하는 Thallium stress test 결과, normal = 보통; fixed\t = 해결된 결함; reversible\t = 해결가능한 결함</center> | 범주형 | 문자열\n",
    ">Target | <center>심장병 진단 (1 = true; 0 = false)</center> | 분류 | 정수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCEhSZcULZ9n"
   },
   "source": [
    "## 2. 판다스로 데이터프레임 만들기\n",
    "\n",
    "[판다스](https://pandas.pydata.org/)는 정형 데이터를 읽고 조작하는데 유용한 유틸리티 함수를 많이 제공하는 파이썬 라이브러리입니다. 판다스를 이용해 URL로부터 데이터를 다운로드하여 읽어 데이터프레임 형식의 데이터로 변환하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REZ57BXCLdfG"
   },
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/applied-dl/heart.csv'\n",
    "dataframe = pd.read_csv(URL)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0zhLtQqMPem"
   },
   "source": [
    "## 3. 데이터프레임을 훈련 세트, 검증 세트, 테스트 세트로 나누기\n",
    "\n",
    "하나의 CSV 파일에서 데이터셋을 다운로드했습니다. 이를 **훈련 세트, 검증 세트, 테스트 세트**로 나누겠습니다.\n",
    "\n",
    "훈련 세트는 전체 데이터 셋 중 80%, 테스트 세트는 전체 데이터 셋 중 20%를 사용하며, 검증 세트는 나눈 훈련 세트의 20%를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEOpw7LhMYsI"
   },
   "outputs": [],
   "source": [
    "print(\"데이터프레임의 shape : \",dataframe.shape)\n",
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "print('훈련 샘플 : %d개' %len(train))\n",
    "print('검증 샘플 : %d개' %len(val))\n",
    "print('테스트 샘플 : %d개' %len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84ef46LXMfvu"
   },
   "source": [
    "## 4. tf.data를 사용하여 입력 파이프라인 만들기\n",
    "\n",
    "그다음 [tf.data](https://www.tensorflow.org/guide/datasets)를 사용하여 데이터프레임을 감싸겠습니다. 이렇게 하면 피쳐 열(`feature_column`)을 사용하여 판다스 데이터프레임의 열을 모델 훈련에 필요한 피쳐로 **매핑**할 수 있습니다. 만약 아주 큰 CSV 파일(메모리에 들어갈 수 없을 정도로 큰 파일)을 다룬다면 tf.data로 디스크 디렉토리에서 데이터를 읽어올 수 있습니다. 그러나 이번 튜토리얼에서 사용하는 CSV 파일은 303개의 행을 가진 작은 파일이기 때문에 디스크 디렉토리에서 읽어올 필요가 없습니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NkcaMYP-MsRe"
   },
   "outputs": [],
   "source": [
    "# 판다스 데이터프레임으로부터 tf.data 데이터셋을 만들기 위한 함수\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXbbXkJvMy34"
   },
   "outputs": [],
   "source": [
    "batch_size = 5 # 예제를 위해 작은 배치 크기를 사용합니다.\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRLGSMDzM-dl"
   },
   "source": [
    "## 5. 입력 파이프라인 이해하기\n",
    "\n",
    "앞서 만든 입력 파이프라인을 호출하여 반환되는 데이터 포맷을 확인해 보겠습니다. 간단한 출력을 위해 작은 배치 크기를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSBo3dUVNFc9"
   },
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print('전체 피쳐:', list(feature_batch.keys()))\n",
    "    print('나이 피쳐의 배치:', feature_batch['age'])\n",
    "    print('타깃의 배치:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT5N6Se-NQsC"
   },
   "source": [
    "이 데이터셋은 데이터프레임의 열 이름을 키로 갖는 딕셔너리를 반환합니다. 딕셔너리의 각 키(key)에는 데이터프레임 열의 값이 매핑되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttIvgLRaNoOQ"
   },
   "source": [
    "## 6. 여러 종류의 피쳐 열(feature_column) 알아 보기\n",
    "\n",
    "텐서플로는 여러 종류의 피쳐 열을 제공합니다. 다음과 같이 몇 가지 피쳐 열을 만들어서 데이터프레임의 열을 변환하는 방법을 알아 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxwiHFHuNhmf"
   },
   "outputs": [],
   "source": [
    "# 피쳐 열을 시험해 보기 위해 샘플 배치를 만듭니다.\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wfLB8Q3N3UH"
   },
   "outputs": [],
   "source": [
    "# 피쳐 열을 만들고 배치 데이터를 변환하는 함수\n",
    "def demo(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7OEKe82N-Qb"
   },
   "source": [
    "### 6.1 수치형 열\n",
    "\n",
    "피쳐 열의 출력은 모델의 입력이 됩니다. 앞서 정의한 함수를 사용하여 데이터프레임의 각 열이 어떻게 변환되는지 알아보겠습니다. [수치형 열](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column)은 가장 간단한 종류의 열로, 이 열은 **실수 피쳐**를 표현하는데 사용됩니다. 이 열을 사용하면 모델은 데이터프레임 열의 값을 변형시키지 않고 그대로 전달 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZTZ0HnHOCxC"
   },
   "outputs": [],
   "source": [
    "age = feature_column.numeric_column(\"age\")\n",
    "demo(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7a6ddSyzOKpq"
   },
   "source": [
    "심장병 데이터셋 데이터프레임의 대부분 열은 이러한 수치형 열 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcSxUoYgOlA1"
   },
   "source": [
    "### 6.2 버킷형 열\n",
    "\n",
    "종종 모델에 수치 값을 바로 주입하는 대신 수치 값의 구간을 나누어 이를 기반으로 **범주형**으로 변환할 필요가 있습니다. 원본 데이터가 사람의 나이를 표현한다고 가정해 보죠. 나이를 수치형 열로 표현하는 대신 [버킷형 열](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column)(bucketized column)을 사용하여 나이를 몇 개의 버킷(bucket)으로 분할할 수 있습니다. 다음의 원-핫 인코딩(one-hot encoding)된 값은 각 열이 매칭되는 나이 범위를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJ4Wt3SAOpTQ"
   },
   "outputs": [],
   "source": [
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "demo(age_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1tArzewPb-b"
   },
   "source": [
    "### 6.3 범주형 열\n",
    "\n",
    "이 데이터셋에서 `thal` 열은 문자열입니다(예를 들어 'fixed', 'normal', 'reversible'). 문자열은 모델에 바로 주입할 수 없기 때문에 대신 문자열을 먼저 수치형으로 매핑해야 합니다. 범주형 열(categorical column)을 사용하여 문자열을 원-핫 벡터로 표현할 수 있습니다. 문자열 목록은 [`categorical_column_with_vocabulary_list`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list)를 사용하여 리스트로 전달하거나 [`categorical_column_with_vocabulary_file`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file)을 사용하여 파일에서 읽을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ6QnSHkPtOC"
   },
   "outputs": [],
   "source": [
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "demo(thal_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxQloQ9jOoXL"
   },
   "source": [
    "더 복잡한 데이터셋에는 범주형(예를 들면 문자열)인 열이 많을 수 있습니다. 피쳐 열은 특히 이러한 범주형 데이터를 다룰 때 진가가 발휘됩니다. 이 데이터셋에는 범주형 열이 하나 뿐이지만 다른 데이터셋에서 사용할 수 있는 여러 종류의 피쳐 열을 소개하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEFPjUr6QmwS"
   },
   "source": [
    "### 6.4 임베딩 열\n",
    "\n",
    "가능한 문자열이 몇 개가 있는 것이 아니라 범주마다 수천 개 이상의 값이 있는 경우를 상상해 보겠습니다. 여러 가지 이유로 범주의 개수가 늘어남에 따라 원-핫 인코딩으로 변환하게 된다면, 고차원 [희소 행렬(sparse matrix)](https://ko.wikipedia.org/wiki/%ED%9D%AC%EC%86%8C%ED%96%89%EB%A0%AC)이 되어 신경망을 잘 훈련시키기 어려워집니다. 그러나 **임베딩 열(embedding column)** 을 사용하면 이런 제한을 극복할 수 있습니다. 고차원 원-핫 벡터로 데이터를 표현하는 대신 [임베딩 열](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column)을 사용하여 저차원으로 데이터를 표현합니다. 이 벡터는 0 또는 1이 아니라 각 원소에 어떤 숫자도 넣을 수 있는 밀집 벡터(dense vector)입니다. 임베딩의 크기(아래 예제에서는 8입니다)는 튜닝 대상 파라미터입니다.\n",
    "\n",
    "- 핵심 포인트: 범주형 열에 가능한 값이 많을 때는 임베딩 열을 사용하는 것이 최선입니다. 여기에서는 예시를 목적으로 하나를 사용하지만 나중에 다른 데이터셋에도 수정하여 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSlohmr2Q_UU"
   },
   "outputs": [],
   "source": [
    "# 임베딩 열의 입력은 앞서 만든 범주형 열입니다.\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "demo(thal_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urFCAvTVRMpB"
   },
   "source": [
    "### 6.5 해시 피쳐 열\n",
    "\n",
    "가능한 값이 많은 범주형 열을 표현하는 또 다른 방법은 [`categorical_column_with_hash_bucket`](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket)을 사용하는 것입니다. 이 피쳐 열은 입력의 해시(hash) 값을 계산한 다음 `hash_bucket_size` 크기의 버킷 중 하나를 선택하여 문자열을 인코딩합니다. 이 열을 사용할 때는 어휘 목록을 제공할 필요가 없고 공간을 절약하기 위해 실제 범주의 개수보다 훨씬 작게 해시 버킷(bucket)의 크기를 정할 수 있습니다.\n",
    "\n",
    "- 핵심 포인트: 이 기법의 큰 단점은 다른 문자열이 같은 버킷에 매핑될 수 있다는 것입니다. 그럼에도 실전에서는 일부 데이터셋에서 잘 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHU_Aj2nRRDC"
   },
   "outputs": [],
   "source": [
    "thal_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "      'thal', hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(thal_hashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fB94M27DRXtZ"
   },
   "source": [
    "### 6.6 교차 피쳐 열\n",
    "\n",
    "여러 피쳐를 연결하여 하나의 피쳐로 만드는 것을 교차 피쳐(feature cross)이라고 합니다. 교차 피쳐를 통해 모델이 피쳐의 조합에 대한 가중치를 학습할 수 있습니다. 이 예제에서는 `age`와 `thal`의 교차 피쳐를 만들어 보겠습니다. `crossed_column`은 모든 가능한 조합에 대한 해시 테이블을 만들지 않고 `hashed_column` 매개변수를 사용하여 해시 테이블의 크기를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaPVERd9Rep6"
   },
   "outputs": [],
   "source": [
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "demo(feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypkI9zx6Rj1q"
   },
   "source": [
    "## 7. 사용할 열 선택하기\n",
    "\n",
    "여러 가지 피쳐 열을 사용하는 방법을 보았으므로 이제 이를 사용하여 모델을 훈련하겠습니다. 이 튜토리얼의 목적은 피쳐 열을 사용하는 완전한 코드(예를 들면 작동 방식)를 제시하는 것이므로 임의로 몇 개의 열을 선택하여 모델을 훈련하겠습니다.\n",
    "\n",
    "- 핵심 포인트: 제대로 된 모델을 만들어야 한다면 대용량의 데이터셋을 사용하고 어떤 피쳐를 포함하는 것이 가장 의미있는지, 또 어떻게 표현해야 할지 신중하게 생각하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PlLY7fORuzA"
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# 수치형 열\n",
    "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# 버킷형 열\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# 범주형 열\n",
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "feature_columns.append(thal_one_hot)\n",
    "\n",
    "# 임베딩 열\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "feature_columns.append(thal_embedding)\n",
    "\n",
    "# 교차 피쳐 열\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-nDp8krS_ts"
   },
   "source": [
    "### 7.1 피쳐 레이어 만들기\n",
    "\n",
    "피쳐 열을 정의하고 나면 [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) 레이어를 사용해 케라스 모델에 주입할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o-El1R2TGQP"
   },
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cf6vKfgTH0U"
   },
   "source": [
    "앞서 피쳐 열의 작동 예를 보이기 위해 작은 배치 크기(5)를 사용했습니다. 여기에서는 조금 더 큰 배치 크기(32)로 입력 파이프라인을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcemszoGSse_"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBx4Xu0eTXWq"
   },
   "source": [
    "## 8. 모델 생성, 컴파일, 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YJPPb3xTPeZ"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnFmMOW0Tcaa"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"정확도\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bdfbq20V6zu"
   },
   "source": [
    "- 핵심 포인트: 일반적으로 크고 복잡한 데이터셋일 경우 딥러닝 모델에서 최선의 결과를 얻습니다. 이런 작은 데이터셋에서는 기본 모델로 **결정 트리(decision tree)** 나 **랜덤 포레스트(random forest)** 를 사용하는 것이 권장됩니다. 이 튜토리얼의 목적은 정확한 모델을 훈련하는 것이 아니라 정형 데이터를 다루는 방식을 설명하는 것입니다. 실전 데이터셋을 다룰 때 이 코드를 시작점으로 사용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNdWfPXCjTjY"
   },
   "source": [
    "# Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "I1dUQ0GejU8N"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "feature_columns.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/ko/tutorials/structured_data/feature_columns.ipynb",
     "timestamp": 1578984188457
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
